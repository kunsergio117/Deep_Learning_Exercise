{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "The process of concerting human speech to text that computers can understand.\n",
    "\n",
    "This course will focus on text, but application to speech is possible. \n",
    "\n",
    "chatgpt is using \"question-answering\"\n",
    "\n",
    "2 sections to NLP:\n",
    "\n",
    "analyze\n",
    "\n",
    "    storing in an abstract form that algorithms can act on.\n",
    "synthesize\n",
    "\n",
    "    forming human speech/text in response.\n",
    "\n",
    "## Analyze\n",
    "\n",
    "### Information Extraction (IE)\n",
    "\n",
    "extracting info from an invoice for example, or any pdf. They can come in different templates and formats, hence the need for AI to adapt to this. \n",
    "\n",
    "1. Rule-based\n",
    "   1. learn extraction rules.\n",
    "2. Classification-based\n",
    "   1. use machine learning techniques to create tokens based on text.\n",
    "   2. treat the IE task as a classfication task.\n",
    "   3. assign classes to each token.\n",
    "   4. combine consecutive tokens into entities.\n",
    "\n",
    "### Information Retrieval\n",
    "\n",
    "google search engine for example, always asking if information is relevant. \n",
    "\n",
    "1. Vector Space Model\n",
    "   1. word embedding\n",
    "   2. represent documents and query as vectors\n",
    "   3. calculate cosine similarity btw the vectors\n",
    "      1. the words \"king\" and \"queen\" are similar but different, and their vectors should reflect this. \n",
    "   4. vector of the dcument should be similar to the vector of the query: to achieve relevance\n",
    "   5. rank the documents based on the similarity value btw document and query and choose top n most similar documents\n",
    "   \n",
    "### Language Detection and Machine Translation (MT)\n",
    "\n",
    "google translate\n",
    "\n",
    "1. Rule-based\n",
    "   1. make use of linguistic rules and bilingual dictionaries for each language pair\n",
    "      1. for example english uses (SVO) subject, Verb, Object\n",
    "2. Statistical-based \n",
    "   1. make use of multilingual Corpa\n",
    "   2. European parliament minutes are all written in all the EU languages\n",
    "   3. does not need to know the grammar of the languages\n",
    "\n",
    "### Text Summarization \n",
    "\n",
    "extractive text summarization\n",
    "\n",
    "abstractive text summmarization\n",
    "\n",
    "### Question Answering (QA)\n",
    "\n",
    "chatgpt is using \"question-answering\"\n",
    "\n",
    "QA components:\n",
    "\n",
    "1. question parser\n",
    "2. query generator\n",
    "3. seracher \n",
    "4. answer candidates extractor\n",
    "5. answers ranker\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document/Text Classification\n",
    "\n",
    "spam detection\n",
    "\n",
    "categorizing the topic of a document\n",
    "\n",
    "sentiment analysis\n",
    "\n",
    "achieving it by:\n",
    "1. convert text into a structured form\n",
    "   1. feature engineering\n",
    "   2. tokenization \n",
    "   3. feature vector generation\n",
    "2. apply ML classification on the 'feature vector'\n",
    "\n",
    "basically we create a table of values that measure y values such as spam based on many many hundreds of x values.\n",
    "\n",
    "### Image captioning\n",
    "\n",
    "uses both NLP and computer vision to generate the captions.\n",
    "\n",
    "the captions extract clues as to what the image may be of, such as text available or shapes or faces present. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "1. Lexical analysis \n",
    "   1. understanding the structure & meaning of words\n",
    "   2. sentence segmentation\n",
    "   3. tokenization\n",
    "   4. text normalization (stemming, lemmatization)\n",
    "2. syntactic anaylsis\n",
    "   1. organization of words based on their structure/ordering in th sentence\n",
    "   2. part of speech (POS) tagging: tagging the words as nouns, verbs, adjectives. \n",
    "   3. shallow parsing (chunking)\n",
    "   4. dependency parsing: trying to grab the relationships between the parts of the sentence\n",
    "3. semantic analysis\n",
    "   1. understanding meaning of words, as the same word can mean different things based on context\n",
    "   2. word sense disambiguation (WSD)\n",
    "   3. Named entity recognition (NER)\n",
    "   4. Semantic Role Labeling (SRL)\n",
    "4. discourse analysis\n",
    "   1. trying to figure out meaning or context from consecutive sentences, as the context and meaning carries on to the next sentence as well as evolves. who does \"he\" refer to? need to refer to previous sentences \n",
    "   2. coreference resolution\n",
    "   3. elipsis resolution\n",
    "      1. inferring the ellipses using the surrounding context\n",
    "      2. ellipsis = words appear to be missing (e.g. lauren can play something, but i don't know what...)\n",
    "5. pragmatic analysis\n",
    "   1. understanding the interpretation of language in the real world context\n",
    "   2. textual entailment\n",
    "      1. determining the directional relation btw text fragments.\n",
    "      2. text entailing the hypothesis (a human that reads t would infer that h is true)\n",
    "   3. intent recognition\n",
    "      1. determining the intent of the written/spoken input\n",
    "      2. turn user words into actions (\"remind me to feed the dog tomorrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
